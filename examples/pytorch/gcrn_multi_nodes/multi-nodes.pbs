#PBS -N gru-gcn-la-feat32-layer2-node8-gpu16-opt-msg
#PBS -A GT-tkim92
#PBS -q embers
#PBS -l nodes=8:ppn=4:gpus=2
#PBS -j oe
#PBS -o gru-gcn-la-feat32-layer2-node8-gpu16-opt-msg.out
#PBS -m abe
#PBS -M mingyu.guan@gatech.edu

cd $PBS_O_WORKDIR
echo $PBS_O_WORKDIR

module load anaconda3/2021.05
conda activate gnn

MASTER=`/bin/hostname -s`
cat $PBS_NODEFILE>nodelist_6
SLAVES=`cat nodelist_6 | grep -v $MASTER | uniq`

HOSTLIST="$MASTER $SLAVES"

MPORT=1234

RANK=0
for node in $HOSTLIST; do
        ssh -q $node ~/.conda/envs/gnn/bin/python -m torch.distributed.launch --nproc_per_node=2 --nnodes=8 --node_rank=$RANK --master_addr="$MASTER" --master_port=1234 $PBS_O_WORKDIR/train.py --gpu 0 --dataset LA --model gcn --rnn gru --num-layers 2 --in-feats 32 --reuse-msg-passing --workspace "$PBS_O_WORKDIR" &
        RANK=$((RANK+1))
done
wait
